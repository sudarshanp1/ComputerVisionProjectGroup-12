# -*- coding: utf-8 -*-
"""rescuenet-Zahid.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uRaNHz_V27XE87wAOgGTJqrGFlT6MB-y
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Conv2D, Conv2DTranspose, MaxPooling2D,\
                             Dropout, Layer, Input, Activation
from tensorflow.keras.models import Model
from tensorflow.keras.utils import Sequence
from tensorflow.keras.optimizers.schedules import ExponentialDecay
import tensorflow.image as tfimg

import pandas as pd
import matplotlib.pyplot as plt
import os
import cv2
import math
import random


img_size = (480, 360)

batch_size = 16
learning_rate = 1e-6


dataset_directory_common = '/content/drive/MyDrive/RescueNet'
dataset_directory_train = dataset_directory_common+'/train/'
dataset_directory_val = dataset_directory_common+'/val/'
dataset_directory_test = dataset_directory_common+'/test/'

# The directory to save the model weights
weights_save_location = '/content/drive/MyDrive/RescueNet/weights'

def categorical_encoding_to_prob(img, num_of_classes):
    res_img = np.zeros((img.shape[0], img.shape[1], num_of_classes))

    for i in range(img.shape[0]):
        for j in range(img.shape[1]):
            if len(img.shape) == 2:
                index = img[i][j]
            else:
                index = img[i][j][0]
            res_img[i][j][index] = 1

    return res_img

def rgb_to_categorical(img, colors):
    res_img = np.zeros((img.shape[0],img.shape[1]), dtype=int)

    for i in range(img.shape[0]):
        for j in range(img.shape[1]):
            if tuple(img[i][j]) in colors:
                res_img[i][j] = colors.index(tuple(img[i][j]))
            else:
                res_img[i][j] = 0

    return res_img

class DirectoryFlow(Sequence):

    def __init__(self, x_dir_path, y_dir_path, batch_size, img_size, img_ext='.jpg', label_suffix_ext='_lab.png', skip_encoding=False, is_rgb=False, colors=None):

        self.x_dir_path = x_dir_path
        self.y_dir_path = y_dir_path

        x_list = os.listdir(x_dir_path)
        y_list = os.listdir(y_dir_path)

        self.data_names_list = [x_el.split('.')[0] for x_el in x_list if x_el.split('.')[0]+label_suffix_ext]
        self.temp_data_names = self.data_names_list.copy()
        self.batch_size = batch_size
        self.img_size = img_size
        self.img_ext = img_ext
        self.label_suffix_ext = label_suffix_ext
        self.skip_encoding = skip_encoding
        self.is_rgb = is_rgb
        self.colors = colors

    def _get_image_batch(self, allow_duplicates=False):

        images = []
        labels = []
        if allow_duplicates:
            image_names_list = np.random.choice(self.temp_data_names, (self.batch_size))
        else:
            image_names_list = random.sample(self.temp_data_names, (self.batch_size))

        self.temp_data_names = [x for x in self.temp_data_names if x not in image_names_list]

        for i_name in image_names_list:
            image = cv2.imread(self.x_dir_path + '/' + i_name + self.img_ext)
            image = cv2.resize(image, self.img_size, interpolation=cv2.INTER_NEAREST)
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            image = image*(1./255)

            label = cv2.imread(self.y_dir_path + '/' + i_name + self.label_suffix_ext)
            label = cv2.resize(label, self.img_size, interpolation=cv2.INTER_NEAREST)
            if not self.skip_encoding:
                if self.is_rgb:
                    label = cv2.cvtColor(label, cv2.COLOR_BGR2RGB)
                    label = rgb_to_categorical(label, colors)
                label = categorical_encoding_to_prob(label, 12)
            else:
                label = cv2.cvtColor(label, cv2.COLOR_BGR2RGB)

            images.append(image)
            labels.append(label)

        return (np.array(images), np.array(labels))

    def __len__(self):
        return int(math.floor(len(self.data_names_list)/self.batch_size))


    def __getitem__(self, index):

        return self._get_image_batch()

    def reset_temp_data_names(self):

        self.temp_data_names = self.data_names_list.copy()

    def on_epoch_end(self):
      self.reset_temp_data_names()

class AugDirectoryFlow(DirectoryFlow):

    def __init__(self, x_dir_path, y_dir_path, batch_size, img_size, img_ext='.jpg', label_suffix_ext='_lab.png',\
                 h_flip=False, v_flip=False, rotation_deg=0, zoom_in_scale=1.0, skip_encoding=False, is_rgb=False, colors=None):

        super().__init__(x_dir_path, y_dir_path, batch_size, img_size, img_ext, label_suffix_ext, skip_encoding, is_rgb, colors)
        self.h_flip = h_flip
        self.v_flip = v_flip
        self.rotation_deg = rotation_deg
        if(zoom_in_scale<1.0):
            raise ValueError('Zoom_in_scale is expected to be 1.0 or higher')
            zoom_in_scale = 1.0
        self.zoom_in_scale = zoom_in_scale

    def _apply_augmentations(self,data_pairs):
        images, labels = data_pairs
        for i in range(len(images)):
            if(self.v_flip):
                if(np.random.randint(2)>0):
                    images[i] = cv2.flip(images[i],0)
                    labels[i] = cv2.flip(labels[i],0)
            if(self.h_flip):
                if(np.random.randint(2)>0):
                    images[i] = cv2.flip(images[i],1)
                    labels[i] = cv2.flip(labels[i],1)
            if(self.rotation_deg > 0 or self.zoom_in_scale > 1.0):
                degrees = 0
                if(self.rotation_deg > degrees):
                    degrees = np.random.randint(0,self.rotation_deg+1)

                scale = 1.0
                if(self.zoom_in_scale > scale):
                    scale = np.random.uniform(1.0,self.zoom_in_scale)

                height, width, _ = images[i].shape
                rotation = cv2.getRotationMatrix2D((height/2, width/2),\
                                                   degrees,scale)
                images[i] = cv2.warpAffine(images[i], rotation, (width, height))
                labels[i] = cv2.warpAffine(labels[i], rotation, (width, height))
        return (images, labels)


    def __getitem__(self, index):

        data_pairs = super()._get_image_batch()
        data_pairs = self._apply_augmentations(data_pairs)
        self.reset_temp_data_names()
        return data_pairs

    def reset_temp_data_names(self):

        super().reset_temp_data_names()

    def on_epoch_end(self):
        pass

dataset_classes_names = {'Background':0, 'Debris':1, 'Water':2,\
                         'Building_No_Damage':3, 'Building_Minor_Damage':4,\
                         'Building_Major_Damage':5,\
                         'Building_Total_Destruction':6, 'Vehicle':7, 'Road':8,\
                         'Tree':9, 'Pool':10, 'Sand':11}

colors = [(0,0,0), (151,0,255), (30,230,255), (184,115,117), (216,255,0), (252,199,0),\
         (255,0,0), (255,0,246), (140,140,140), (0,255,0), (244,255,0), (152,166,0)]

def class_code_to_rgb(image):
    image = image.numpy()
    result_img = np.zeros((image.shape[0], image.shape[1], 3), dtype=int)
    for i in range(image.shape[0]):
        for j in range(image.shape[1]):
            px_class = image[i][j]
            result_img[i][j][0] = px_class*20
            result_img[i][j][1] = px_class*20
            result_img[i][j][2] = px_class*20
    return result_img

def class_code_to_rgb_w_colors(image, colors, use_np=False):
    if not use_np:
        image = image.numpy()
    result_img = np.zeros((image.shape[0], image.shape[1], 3), dtype=int)
    for i in range(image.shape[0]):
        for j in range(image.shape[1]):
            px_class = image[i][j]
            result_img[i][j][0] = colors[px_class][0]
            result_img[i][j][1] = colors[px_class][1]
            result_img[i][j][2] = colors[px_class][2]
    return result_img

dataset_train_org_img = dataset_directory_train+'train-org-img'
print(dataset_train_org_img)

dataset_train_label_image = dataset_directory_train+'train-label-img'

"""## Preview some data from dataset"""

flow = DirectoryFlow(dataset_train_org_img, dataset_train_label_image, batch_size, img_size)

images, labels = flow._get_image_batch()
image = images[0]
label = labels[0]
plt.imshow(image)

"""As an array the picture after normalisation looks like this"""

image

"""Before the normalization it looked like this"""

(image*255).astype(np.int32)

label = tf.argmax(label, axis=-1)
plt.imshow(label)

"""The map itself looks the following way"""

label

"""And current picture has this many unique classes(colors)"""

np.unique(label)

label.shape

print(len(flow.temp_data_names))

flow.reset_temp_data_names()
print(len(flow.temp_data_names))

"""## Model elements definition"""

class SkipConnLayer(Layer):

    def __init__(self, storage, key):

        super(SkipConnLayer, self).__init__()
        self.storage = storage
        self.key = key

    def call(self, inputs):
        self.storage[0][self.key] = tf.identity(inputs)
        return inputs

class AttentionLayer(Layer):

    def __init__(self, storage, key, num_of_filters):

        super(AttentionLayer, self).__init__()
        self.storage = storage
        self.key = key
        self.num_of_filters = num_of_filters
        self.prev_layer_conv = Conv2D(num_of_filters, (1,1), activation='relu',\
                                      kernel_initializer='he_normal')
        self.skip_conv = Conv2D(num_of_filters, (1,1), strides=(2,2),\
                                activation='relu', kernel_initializer='he_normal')
        self.post_add_activation = Activation(activation='relu')

        self.proj_conv = Conv2D(1, (1,1), activation='relu',\
                                kernel_initializer='he_normal')
        self.pre_upsample_activation = Activation(activation='sigmoid')

    def call(self, inputs):
        skip_input = self.storage[0][self.key]

        inputs_temp = self.prev_layer_conv(inputs)
        skip_temp = self.skip_conv(skip_input)
        temp_features = tf.add(inputs_temp, skip_temp)
        temp_features = self.post_add_activation(temp_features)
        temp_features = self.proj_conv(temp_features)
        temp_features = self.post_add_activation(temp_features)
        temp_features = tfimg.resize(temp_features, (skip_input.shape[1],\
                                                     skip_input.shape[2]),\
                                     method='bilinear')
        return tf.math.multiply(skip_input, temp_features)

class MyMeanIOU(tf.keras.metrics.MeanIoU):
    def update_state(self, y_true, y_pred, sample_weight=None):
        return super().update_state(tf.argmax(y_true, axis=-1), tf.argmax(y_pred, axis=-1), sample_weight)

skip_conn_data = np.array([{}])

class UNetSegmentModelOld:


    def create_model(self, filter_number_list, initial_shape):
        input = Input(initial_shape)
        x = input
        for num_of_filters in filter_number_list[:-1]:
            x = Conv2D(num_of_filters, (3,3), padding='same',\
                       activation='relu', kernel_initializer='he_normal')(x)
            x = Dropout(0.05)(x)

            x = Conv2D(num_of_filters, (3,3), padding='same',\
                             activation='relu', kernel_initializer='he_normal')(x)
            x = SkipConnLayer(skip_conn_data, f'data_{num_of_filters}')(x)
            x = MaxPooling2D((2,2))(x)


        x = Conv2D(filter_number_list[-1], (3,3), padding='same',\
                   activation='relu', kernel_initializer='he_normal')(x)
        x = Dropout(0.05)(x)
        x = Conv2D(filter_number_list[-1], (3,3), padding='same',\
                   activation='relu', kernel_initializer='he_normal')(x)

        for num_of_filters in reversed(filter_number_list[:-1]):

            attention_x = AttentionLayer(skip_conn_data,\
                                            f'data_{num_of_filters}', x.shape[3])(x) #skip_conn_data[0][f'data_{num_of_filters}']
            x = Conv2DTranspose(num_of_filters, (2,2), strides=(2,2),\
                                padding='same')(x)
            x = tf.keras.layers.concatenate([attention_x, x], -1)
            x = Conv2D(num_of_filters, (3,3), padding='same',\
                       activation='relu', kernel_initializer='he_normal')(x)
            x = Dropout(0.05)(x)
            x = Conv2D(num_of_filters, (3,3), padding='same',\
                       activation='relu', kernel_initializer='he_normal')(x)

        output = Conv2D(len(dataset_classes_names), (1,1), padding='same',\
                        activation='softmax')(x)
        self.model = Model(inputs=input, outputs=output)

    def compile_model(self, base_lr, use_exponential_decay=False, decay_rate=0.0, decay_steps=0):

        optimizer = None
        if use_exponential_decay:
            learning_rate = ExponentialDecay(base_lr, decay_rate = decay_rate,\
                                             decay_steps = decay_steps)
            optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
        else:
            optimizer = tf.keras.optimizers.Adam(learning_rate=base_lr)

        self.model.compile(optimizer=optimizer, loss='categorical_crossentropy',\
                      metrics = ['accuracy', MyMeanIOU(num_classes=len(dataset_classes_names))])
        return self.model

"""## Network creation and fitting"""

unetObj = UNetSegmentModelOld()
unetObj.create_model([64, 128, 256, 512], (img_size[1], img_size[0], 3))
model = unetObj.compile_model(learning_rate, True, 0.5, 3200)
train_flow = AugDirectoryFlow(dataset_directory_train+'train-org-img',\
                              dataset_directory_train+'train-label-img',\
                              batch_size, img_size, h_flip=True, v_flip=True,\
                              zoom_in_scale=1.3) #rotation_deg=90,
val_flow = DirectoryFlow(dataset_directory_val+'val-org-img',\
                         dataset_directory_val+'val-label-img',\
                         batch_size, img_size)

if(not os.path.isdir(weights_save_location)):
    os.makedirs(weights_save_location)

checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(weights_save_location, 'model_weights.weights.h5'),
                                                         monitor='val_loss',
                                                         save_best_only=True,
                                                         save_weights_only=True)


train_steps = 32
val_steps = len(val_flow)

"""<b>Optional part:</b> loading the weights"""

if(len(os.listdir(weights_save_location)) != 0):
  model.load_weights('/content/drive/MyDrive/RescueNet/weights/model_weights.weights.h5')
  print("Weights loaded successfully!")

"""Training init call"""

model.fit(train_flow, steps_per_epoch=train_steps, validation_data=val_flow, validation_steps=val_steps, epochs=1, verbose=1, callbacks=[checkpoint_callback])

"""## Testing out the resulting weights"""

test_flow = DirectoryFlow(dataset_directory_test+'test-org-img', dataset_directory_test+'test-label-img', 2, img_size)
(image, label) = test_flow._get_image_batch()

predicted_map = model.predict(image)

#orig picture
plt.imshow(image[0])

predicted_map_val = tf.argmax(predicted_map[0], axis=-1)
plt.imshow(predicted_map_val)

#ground truth
label_val = tf.argmax(label[0], axis=-1)
plt.imshow(label_val)

np.unique(predicted_map_val)

np.unique(label_val)

plt.imshow(class_code_to_rgb(predicted_map_val))

plt.imshow(class_code_to_rgb(label_val))

test_flow = DirectoryFlow(dataset_directory_test+'test-org-img',\
                          dataset_directory_test+'test-label-img', 3, img_size)
(image, label) = test_flow._get_image_batch()
predicted_map = model.predict(image)

fig = plt.figure(figsize=(10,10))

for i in range(3):
    fig.add_subplot(3, 3, i*3+1)
    plt.imshow(image[i])

    fig.add_subplot(3, 3, i*3+2)
    plt.imshow(class_code_to_rgb_w_colors(tf.argmax(label[i], axis=-1), colors))

    fig.add_subplot(3, 3, i*3+3)
    plt.imshow(class_code_to_rgb_w_colors(tf.argmax(predicted_map[i], axis=-1), colors))

plt.show()

class_names_list = list(dataset_classes_names)
num_of_classes = len(dataset_classes_names)
test_flow = DirectoryFlow(dataset_directory_test+'test-org-img',\
                          dataset_directory_test+'test-label-img', batch_size, img_size)

iou_arr = []
for class_num in range(num_of_classes):
    iou_arr.append(tf.keras.metrics.IoU(num_classes=num_of_classes, target_class_ids=[class_num]))



for _ in range(len(test_flow)):
    (images, labels) = test_flow._get_image_batch()
    predictions = model.predict(images, verbose=0)
    for i in range(len(predictions)):
        prediction = tf.argmax(predictions[i], axis=-1)
        label = tf.argmax(labels[i], axis=-1)
        for class_num in range(num_of_classes):
            class_label = label
            class_pred = prediction
            iou_arr[class_num].update_state(class_label, class_pred)



for i, iou in enumerate(iou_arr):
    print(f'Class: {class_names_list[i]}, IoU: {iou.result()}')
test_flow.reset_temp_data_names()

import shutil
shutil.make_archive('AUNet_weights', 'zip', '/kaggle/working/RescueAUNet')

"""## Testing on a data from the extension dataset"""



ext_dataset_path = '/kaggle/input/rescuenetextension/RescueNetExtensionsLabelid/'

unetObj = UNetSegmentModelOld()
unetObj.create_model([64, 128, 256, 512], (img_size[1], img_size[0], 3))
model = unetObj.compile_model(learning_rate)

if(len(os.listdir(weights_save_location)) != 0):
    model.load_weights('/content/drive/MyDrive/RescueNet/weights/model_weights.weights.h5')
    print("Weights loaded successfully!")

test_flow = DirectoryFlow(ext_dataset_path+'test-org-img',\
                          ext_dataset_path+'test-label-color-img', 3, img_size, img_ext='.png',\
                          label_suffix_ext='_lab_col.png', skip_encoding=True)
(image, label) = test_flow._get_image_batch()
predicted_map = model.predict(image)

fig = plt.figure(figsize=(10,10))

for i in range(3):
    fig.add_subplot(3, 3, i*3+1)
    plt.imshow(image[i])

    fig.add_subplot(3, 3, i*3+2)
    plt.imshow(label[i])

    fig.add_subplot(3, 3, i*3+3)
    plt.imshow(class_code_to_rgb_w_colors(tf.argmax(predicted_map[i], axis=-1),colors))

plt.show()

class_names_list = list(dataset_classes_names)
num_of_classes = len(dataset_classes_names)
test_flow = DirectoryFlow(ext_dataset_path+'test-org-img',\
                          ext_dataset_path+'test-label-color-img', 3, img_size, img_ext='.png',\
                          label_suffix_ext='_lab_col.png', is_rgb=True, colors=colors)

iou_arr = []
for class_num in range(num_of_classes):
    iou_arr.append(tf.keras.metrics.IoU(num_classes=num_of_classes, target_class_ids=[class_num]))



for _ in range(len(test_flow)):
    (images, labels) = test_flow._get_image_batch()
    predictions = model.predict(images, verbose=0)
    for i in range(len(predictions)):
        prediction = tf.argmax(predictions[i], axis=-1)
        label = tf.argmax(labels[i], axis=-1)
        for class_num in range(num_of_classes):
            class_label = label
            class_pred = prediction
            iou_arr[class_num].update_state(class_label, class_pred)



for i, iou in enumerate(iou_arr):
    print(f'Class: {class_names_list[i]}, IoU: {iou.result()}')
test_flow.reset_temp_data_names()
