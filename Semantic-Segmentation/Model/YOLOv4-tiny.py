# -*- coding: utf-8 -*-
"""ComputerVisionProjectGroup12!.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bYYVKyn7A0BqUgTJRJmzSaytXDOX3gFo

### Disaster-Resilient Military Base Damage Assessment with Autonomous Object Tracking

1. Data Loading and Visualization:
"""

import os
import cv2
import matplotlib.pyplot as plt

# Mount Google Drive (if needed)
from google.colab import drive
drive.mount('/content/drive')

# Path to your dataset
data_path = '/content/drive/MyDrive/RescueNet' #change this to your path.

# Load a sample image and its label mask
train_org_img_path = os.path.join(data_path, 'train/train-org-img')
train_label_img_path = os.path.join(data_path, 'train/train-label-img')

sample_image_file = os.listdir(train_org_img_path)[0]
sample_label_file = os.listdir(train_label_img_path)[0]

sample_image = cv2.imread(os.path.join(train_org_img_path, sample_image_file))
sample_label = cv2.imread(os.path.join(train_label_img_path, sample_label_file))

# Convert BGR to RGB for matplotlib
sample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)
sample_label = cv2.cvtColor(sample_label, cv2.COLOR_BGR2RGB)

# Display the image and label mask
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.imshow(sample_image)
plt.title('Sample Image')
plt.subplot(1, 2, 2)
plt.imshow(sample_label)
plt.title('Label Mask')
plt.show()

"""2. YOLOv4-Tiny Implementation (using ultralytics library):"""

import yaml

data = {
    'train': '/content/drive/MyDrive/RescueNet/train',
    'val': '/content/drive/MyDrive/RescueNet/val',
    'test': '/content/drive/MyDrive/RescueNet/test',
    'nc': 2,  # Replace with the actual number of classes
    'names': ['damage', 'non-damage']  # Replace with your actual class names
}

yaml_file_path = '/content/drive/MyDrive/RescueNet/data.yaml'

with open(yaml_file_path, 'w') as outfile:
    yaml.dump(data, outfile, default_flow_style=False)

print(f"data.yaml created at: {yaml_file_path}")

!pip install ultralytics

from ultralytics import YOLO

# Load YOLOv4-tiny model
model = YOLO('yolov8n.pt') #You can also load a custom trained model.

# Train the model (adjust paths and parameters)
results = model.train(data=os.path.join(data_path, 'data.yaml'), epochs=10, imgsz=640) #data.yaml needs to be created, and contain the path to the train and test images.

#Predict
results = model.predict(source=os.path.join(data_path, 'test/test-org-img'), save=True)

import os
import cv2
import matplotlib.pyplot as plt

# ... (Mount Google Drive and define data_path) ...

train_org_img_path = os.path.join(data_path, 'train/train-org-img')
train_label_img_path = os.path.join(data_path, 'train/train-label-img')

image_files = os.listdir(train_org_img_path)[:4]  # Get the first 4 images
label_files = os.listdir(train_label_img_path)[:4]

plt.figure(figsize=(12, 8))
for i in range(4):
    image = cv2.imread(os.path.join(train_org_img_path, image_files[i]))
    label = cv2.imread(os.path.join(train_label_img_path, label_files[i]))
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    label = cv2.cvtColor(label, cv2.COLOR_BGR2RGB)

    plt.subplot(2, 4, i + 1)
    plt.imshow(image)
    plt.title(f'Image {i + 1}')

    plt.subplot(2, 4, i + 5)
    plt.imshow(label)
    plt.title(f'Label {i + 1}')

plt.show()

"""Right, let's create the data.yaml file for YOLOv8 and also address the conversion of segmentation masks (if you have them) to bounding box annotations.

1. Creating data.yaml - The data.yaml file tells YOLOv8 where to find your training and validation data, how many classes you have, and what the class names are. Here's how to create it:


"""

import yaml
import os

# Define your paths and class information
train_images_path = '/content/drive/MyDrive/RescueNet/train'  # Path to training images
val_images_path = '/content/drive/MyDrive/RescueNet/val'    # Path to validation images
test_images_path = '/content/drive/MyDrive/RescueNet/test' #Path to test images.

# Assuming your labels are in the same directories as the images, you might need to adjust this
train_labels_path = '/content/drive/MyDrive/RescueNet/train' #Path to training labels
val_labels_path = '/content/drive/MyDrive/RescueNet/val' #Path to validation labels
test_labels_path = '/content/drive/MyDrive/RescueNet/test' #path to test labels.

# If you have converted the segmentation masks into bounding box annotations,
# put the paths to those text files here.
# Otherwise, you'll need to create those text files.

num_classes = 2  # Replace with the actual number of classes
class_names = ['damage', 'non-damage']  # Replace with your class names

# Create the data dictionary
data = {
    'train': train_images_path,
    'val': val_images_path,
    'test': test_images_path,
    'nc': num_classes,
    'names': class_names
}

# Write the data to a YAML file
yaml_file_path = '/content/drive/MyDrive/RescueNet/data.yaml'
with open(yaml_file_path, 'w') as outfile:
    yaml.dump(data, outfile, default_flow_style=False)

print(f"data.yaml created at: {yaml_file_path}")

"""2. Converting Segmentation Masks to Bounding Boxes (if needed)

If you have segmentation masks, you'll need to convert them into bounding box annotations (text files) that YOLOv8 can use. Here's a Python script to do this:
"""

import cv2
import os

def mask_to_bbox(mask_path, output_path, image_shape):
    """Converts a segmentation mask to a bounding box annotation."""
    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
    if mask is None:
        return None

    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    if not contours:
        return None

    x_min, y_min, x_max, y_max = image_shape[1], image_shape[0], 0, 0
    for contour in contours:
        x, y, w, h = cv2.boundingRect(contour)
        x_min = min(x_min, x)
        y_min = min(y_min, y)
        x_max = max(x_max, x + w)
        y_max = max(y_max, y + h)

    x_center = (x_min + x_max) / 2 / image_shape[1]
    y_center = (y_min + y_max) / 2 / image_shape[0]
    width = (x_max - x_min) / image_shape[1]
    height = (y_max - y_min) / image_shape[0]
    return f"0 {x_center} {y_center} {width} {height}"

def process_masks_to_bboxes(mask_dir, image_dir, output_dir):
    """Processes all masks in a directory and creates corresponding bounding box annotations."""
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    mask_files = os.listdir(mask_dir)
    for mask_file in mask_files:
        mask_path = os.path.join(mask_dir, mask_file)
        image_file = mask_file.replace('-label-img.png', '.png') #change this line according to your file name conventions.
        image_path = os.path.join(image_dir, image_file)

        image = cv2.imread(image_path)
        if image is None:
            continue
        image_shape = image.shape

        bbox_annotation = mask_to_bbox(mask_path, output_dir, image_shape)
        if bbox_annotation:
            txt_file = os.path.splitext(mask_file)[0] + '.txt'
            txt_path = os.path.join(output_dir, txt_file)
            with open(txt_path, 'w') as f:
                f.write(bbox_annotation)

# Example usage:
mask_dir = '/content/drive/MyDrive/RescueNet/train/train-label-img' #path to your mask labels.
image_dir = '/content/drive/MyDrive/RescueNet/train/train-org-img' #path to your original images.
output_dir = '/content/drive/MyDrive/RescueNet/train/train-labels' #path to where you want the bounding box labels to be saved.
process_masks_to_bboxes(mask_dir, image_dir, output_dir)

mask_dir = '/content/drive/MyDrive/RescueNet/test/test-label-img' #path to your mask labels.
image_dir = '/content/drive/MyDrive/RescueNet/test/test-org-img' #path to your original images.
output_dir = '/content/drive/MyDrive/RescueNet/test/test-labels' #path to where you want the bounding box labels to be saved.
process_masks_to_bboxes(mask_dir, image_dir, output_dir)

"""1. Training the YOLOv8 Model

Using Ultralytics:

Ultralytics makes training very straightforward. You'll use the YOLO class and the train() method.
"""

from ultralytics import YOLO

# Load a model
model = YOLO('yolov8n.pt')  # Load a pretrained YOLOv8n model, or use a custom path

# Train the model
model.train(data='/content/drive/MyDrive/RescueNet/data.yaml', epochs=1, imgsz=640)  # Adjust epochs and image size as needed

# Optionally, you can validate the model after training
metrics = model.val(data='/content/drive/MyDrive/RescueNet/data.yaml')  # Evaluate model performance on the validation set

"""1. Analyze Training and Validation Results

Training Logs:

The training process will output logs to your console, showing metrics like loss, precision, recall, and mAP at each epoch.
Pay attention to how these metrics change over epochs. Ideally, you want to see the loss decreasing and the other metrics improving.
"""

print(metrics.box.map)  # mAP for bounding box detection
print(metrics.box.precision)
print(metrics.box.recall)

from ultralytics import YOLO

# Load your trained model
model = YOLO('/content/runs/detect/train/weights/best.pt')  # Replace with the path to your trained model

# Validate the model
metrics = model.val(data='/content/drive/MyDrive/RescueNet/RescueNet/data.yaml')

# Print the metrics
if metrics:
    print(metrics.box.map)  # mAP for bounding box detection
    print(metrics.box.p)    # Precision
    print(metrics.box.r)    # Recall
else:
    print("Validation failed or metrics were not generated.")

""". Test on the Test Set

Run Predictions on Test Images:

Use the model.predict() function to run your trained model on the test set.
"""

results = model.predict(source='/content/drive/MyDrive/RescueNet/RescueNet/test/test-org-img')

"""Analyze Test Results:

Examine the detection results.
If you have test labels, you can use the model.val() function on your test dataset as well, to get the metrics.

"""

metrics_test = model.val(data = '/content/drive/MyDrive/RescueNet/RescueNet/data.yaml', split = 'test')

from ultralytics import YOLO

# Load a pre-trained model (or initialize a new one)
model = YOLO('yolov8n.pt')  # Or another model like yolov8s.pt, yolov8m.pt, etc.

# Train the model
model.train(data='/content/drive/MyDrive/RescueNet/RescueNet/data.yaml', epochs=100, imgsz=640)  # Adjust epochs and image size as needed
